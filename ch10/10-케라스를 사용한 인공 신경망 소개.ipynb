{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10-케라스를 사용한 인공 신경망 소개.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1DPgVT9uJldBfXoJXy2pABnB-yaQxVouo","authorship_tag":"ABX9TyNBH5lIvdNfGT9KSgdH2AQj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QZDCpWVT9aCc"},"source":["# 케라스를 사용한 인공 신경망 소개\n","* 다층 퍼셉트론(MLP)소개\n","* 저수준 API를 이용해서 사용자 정의 케라스 컴포넌트를 만들 수 있다(12장)\n","---\n","## 생물학적 뉴런에서 인공 뉴런까지\n","### 생물학적 뉴런\n","### 뉴런을 사용한 논리연산\n","### 퍼셉트론(TLU,LTU)\n","* 1957년에 프랑크 로젠블라트 제안\n","* 입력의 가중치 합을 계산한 뒤에 계단함수를 적용하여 결과 출력.\n","* 하나의 TLU는 간단한 선형 이진 분류 가능\n","* 모든 뉴런이 연결되면 완전연결층 또는 밀집층.\n","* 입력은 입력층에서 편향특성이 더해저 다음 층으로 간다.\n","* 퍼셉트론은 오차가 감소되도록 연결을 강화한다.\n","* 퍼셉트론은 매개변수가 loss=\"perceptron\",learning_rate=\"constant\",eta0=1,penalty=None인 SGDClassifier와 같다.\n","* 1969년 민스키 교수가 약점을 언급하며 침체기 빠짐!\n","* 퍼셉트론을 여러층 쌓으면 xor문제를 풀수 있다."]},{"cell_type":"code","metadata":{"id":"7xnCAg8-vO_M"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import Perceptron\n","import numpy as np\n","import pandas as pd\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJ049A04-da7"},"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import Perceptron\n","\n","iris=load_iris()\n","x=iris.data[:,(2,3)]\n","y=(iris.target==0).astype(np.int)\n","\n","per_clf=Perceptron()\n","per_clf.fit(x,y)\n","\n","y_pred=per_clf.predict([[2,0.5]])\n","y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prKDo0G2CXrd"},"source":["### 다층 퍼셉트론과 역전파\n","* 입력층 하나와 은닉층이라 불리는 하나 이상의 TLU층과 마지막 출력층으로 구성.\n","* 입력층에 가까운 층을 하위층(lower layer),출력에 가까운 층을( upper layer)라고 한다.\n","* 은닉층을 여러개 쌓아 올린 인공 신경망을 심층 신경망(DNN)이라고 한다.\n","* 1986년 루멜하트,힌턴이 역전파를 소개한다. 효과적으로 그레디언트 계산하는 경사하강법!\n","* 네트워크를 정방향 역방향 한번 통과하는 것으로 오차의 그레디언트를 계산 할 수 있다. 즉 가중치와 편향값을 수정 할 수 있다.\n","* 한번에 하나의 미니배치씩 진행하여 전체 훈련세트를 처리한다. 각 반복을 에포크라고 한다.\n","* 각 미니배치는 네트워크 입력층으로 전달되 첫번째 은닉층으로 보내진다.\n","* 그 다음 해당 층에 있는 모든 뉴런의 출력을 계산하고 다음층으로 전달하고 마지막까지 간다.(정방향 계싼)\n","* 그 다음 출력 오차를 측정한다. 다음 각 출력 연결이 이 오차에 기여하는 정도를 계산한다(미적분의 연쇄법칙이 쉽다).\n","* 오차 그레디언트를 거꾸로 전파함으로 연결이 오차에 기여한 정도를 측정한다.\n","* 시그모이드,하이퍼볼릭탄젠트,relu 활성화함수!\n","* 선형함수 두개를 붙이면 선형이다. 따라서 비선형성이 추가되야해서 활성화 함수를 쓴다.\n","---\n","### 회귀를 위한 다층 퍼셉트론.\n","* 출력 뉴런이 하나다.\n","* 다변량 회귀는 출력 차원마다 출력 뉴런이 하나씩 필요하다. 2D위치를 찾기 위해 출력 2개! 너비와 톺이를 알려면 2개 더!\n","* 출력이 항상 양수이기 위함으로 RELU와 softplus(log(1+exp(z))활성화 함수가 있다.\n","* 어떤 범위 안의 값을 예측하고 싶다면 로지스틱 함수나 하이퍼볼릭 탄젠트가 가능하다.\n","* 손실함수는 보통 MSE이나 이상치가 많으면 평균 절댓값 오차를 사용할 수 있다. 이 둘을 조합한 후버 손실도 있다.\n","  * 후버 손실은 오차가 입계값(보통 1) 보다 작으면 이차 크면 선형함수이다.\n","---\n","### 분류를 위한 다층 퍼셉트론\n","* 이진 분류에서는 하나의 출력뉴런만 필요\n","*  3개 이상의 클래스중 하나만을 선택해야 한다면 소프트맥스를 사용.\n","* 확률 분포를 예측해야 하므로 손실함수는 보통 크로스엔트로피 손실 사용.\n","---\n","### 케라스로 다층 퍼셉트론 구현하기.\n","* 텐서플로와 케라스와 파이토치가 인기있는 딥러닝 라이브러리다!\n"]},{"cell_type":"code","metadata":{"id":"bP2qiKs3ByI8"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","tf.__version__,keras.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Fbb178GQieG"},"source":["fashion_mnist=keras.datasets.fashion_mnist\n","(x_train_full,y_train_full),(x_test,y_test)=fashion_mnist.load_data()\n","x_val,x_train=x_train_full[:5000]/255.0,x_train_full[5000:]/255.0\n","y_val,y_train=y_train_full[:5000],y_train_full[5000:]\n","x_test=x_test/255.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jbm_JQeiQ0Rd"},"source":["x_train_full.shape,x_train_full.dtype"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHga_goKRQ9t"},"source":["class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n","               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObF5WEnIRn01"},"source":["class_names[y_train[0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WR2coeGVRsDl"},"source":["import matplotlib.pyplot as plt\n","n_rows=4\n","n_columns=10\n","plt.figure(figsize=(n_columns*1.2,n_rows*1.2))\n","for row in range(n_rows):\n","  for columns in range(n_columns):\n","    index=n_columns*row+columns\n","    plt.subplot(n_rows,n_columns,index+1)\n","    plt.imshow(x_train[index],cmap=\"binary\",interpolation=\"nearest\")\n","    plt.axis('off')\n","    plt.title(class_names[y_train[index]],fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5uotEFxUGLX"},"source":["### 시퀀셜 API를 사용하여 모델 만들기"]},{"cell_type":"code","metadata":{"id":"eVVE0Hr2T10u"},"source":["model=keras.models.Sequential()                      #시퀀셜 모델을 만든다. 간단한 케라스 신경망 모델이다. 순서대로 쌓는다.\n","model.add(keras.layers.Flatten(input_shape=[28,28])) # 1D배열로 만든다\n","model.add(keras.layers.Dense(300,activation=\"relu\")) # 뉴런 300개를 가진 은닉층 추가한다. Dense층 마다 가중치 행렬을 관리한다.\n","model.add(keras.layers.Dense(120,activation=\"relu\")) # 뉴런 120개를 가진 은닉층 추가\n","model.add(keras.layers.Dense(10,activation=\"softmax\")) # soft맥스를 활성화 함수로 출력층 추가."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"herqh_8xCv09"},"source":["model=keras.models.Sequential([\n","                               keras.layers.Flatten(input_shape=[28,28]),\n","                               keras.layers.Dense(300,activation=\"relu\"),\n","                               keras.layers.Dense(300,activation=\"relu\"),\n","                               keras.layers.Dense(100,activation=\"relu\"),\n","                               keras.layers.Dense(10,activation=\"softmax\")\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7pxAdhdk2h5e"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaP-NfmV3JB8"},"source":["print(model.layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cf8PP9a24kuW"},"source":["* Dense층은 연결 가중치를 무작위로 초기화하고 편향은 0으로 초기화한다.\n","* 다른 초기화 방법을 사용하기 위해 층을 만들 때 kernel_initializer와 bias_initializer 매개변수를 설정할 수 있다."]},{"cell_type":"code","metadata":{"id":"dYyx4oj23crn"},"source":["import numpy as np\n","import pandas as pd\n","weights,biases=model.layers[1].get_weights()\n","weights.shape,biases.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs5BlQ3U5Cc9"},"source":["##### 모델 컴파일\n","* loss 는 손실함수 (이진 분류나 다중 레이블 이진 분류를 수행한다면 출력층에 softmax대신 sigmoid함수를 사용하고 binary_crossentropy를 사용한다.)\n","* 희소한 레이블(클래스의 인덱스)을 원핫 레이블로 변환하려면 keras.utils.categorical()함수를 사용한다!. 반대로 변환하려면 axis=1로 설정하여 np.argmax()를 사용한다.\n","* optimizer를 sgd로 지정하면 확률적 경사 하강법을 사용하는것이다."]},{"cell_type":"code","metadata":{"id":"bE2if1ZZ3xDN"},"source":["model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8OG6caa6Ki2"},"source":["##### 모델 훈련과 평가\n","* fit 메서드 사용.\n","* 입력 특성,타깃클래스, 에포크 횟수(default=1)를 전달한다. 검증세트(선택사항)\n","* validation에 0.1로 쓰면 검증에 마지막 10%를 사용한다.\n","* 훈련세트의 클래스 개수가 편중되어 있다면 fit에 class_weight변수를 지정하는게 좋다. 샘플별로 가중치를 부여한다면 sample_weight 매개변수를 지정한다( 둘다 지정되면 곱한다.)\n","* fit 메서드의 출력인 History객체는 훈련파라미터(history.params), 수행된 에포크 리스트(history.epoch)가 포함된다.\n","* 이 객체의 중요한 속성은 에포크가 끝날때마다 훈련 세트와 검증 세트의 손실과 측정한 지표를 담은 history.history이다. 이것을 통해 판다스 데이터 프레임을 만들고 학습 곡선을 만들 수 있다."]},{"cell_type":"code","metadata":{"id":"_UGK_j4X5Aij"},"source":["history=model.fit(x_train,y_train,epochs=5,batch_size=5,validation_data=(x_val,y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Af-Mha2k8h6k"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","pd.DataFrame(history.history).plot(figsize=(8,5))\n","plt.grid(True)\n","plt.gca().set_ylim(0,1)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmiOPCbv-EhI"},"source":["model.evaluate(x_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s1Aw0RrO--O0"},"source":["### 모델을 사용해 예측을 만들기\n","* predict를 사용해서 예측을하면 각 레이블별 확률이 나온다!"]},{"cell_type":"code","metadata":{"id":"enuPATV8-2-w"},"source":["x_new=x_test[:3]\n","y_proba=model.predict(x_new)\n","y_proba.round(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTA1D9s0_jpX"},"source":["y_pred=model.predict_classes(x_new)\n","np.array(class_names)[y_pred]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eu8bime2_5_h"},"source":["for i,image in enumerate(x_test[:3]):\n","  plt.subplot(1,3,i+1)\n","  plt.imshow(image,cmap=\"binary\")\n","  plt.title(class_names[y_pred[i]])\n","  plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bP3HK0DBUhy"},"source":["### 시퀀셜 API를 사용하여 회귀용 다층 퍼셉트론 만들기\n","* 캘리포니아 주택 가격 데이터셋으로 회귀 신경망으로 해결!\n","* 출력층이 활성화 함수가 없는 하나의 뉴런(하나의 값을 예측하기에) 과 평균 제곱 오차를 사용하는 것.\n","* 이 데이터셋은 잡음이 많기에 과대적합을 막는 용도로 뉴런수가 적은 은닉층 하나만 쓴다."]},{"cell_type":"code","metadata":{"id":"S3Em6MCDArfZ"},"source":["from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","housing=fetch_california_housing()\n","x_train_full,x_test,y_train_full,y_test=train_test_split(housing.data,housing.target)\n","x_train,x_val,y_train,y_val=train_test_split(x_train_full,y_train_full)\n","\n","scaler=StandardScaler()\n","x_train=scaler.fit_transform(x_train)\n","x_val=scaler.transform(x_val)\n","x_test=scaler.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqj3Jz_VCZwX"},"source":["model=keras.models.Sequential([\n","                               keras.layers.Dense(30,activation=\"relu\",input_shape=x_train.shape[1:]),\n","                               keras.layers.Dense(1)\n","])\n","\n","model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n","history=model.fit(x_train,y_train,epochs=10,batch_size=10,validation_data=(x_val,y_val))\n","mse_test=model.evaluate(x_test,y_test)\n","x_new=x_test[:3]\n","y_pred=model.predict(x_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiikdcp8F4Ad"},"source":["pd.DataFrame(history.history).plot()\n","print(y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ac7EXhyOHIU3"},"source":["### 함수형 API를 사용해 복잡한 모델 만들기\n","* 순차적이지 않은 신경망의 한 예는 와이드&딥 신경망이다.\n","* 2016년 헝쯔 청의 논문에서 소개되엇다.\n","* 입력의 일부 또는 전체가 출력층에 바로 연결된다.\n","* 대조적으로 보통 MLP는 모든 층을 데이터가 통과한다."]},{"cell_type":"code","metadata":{"id":"Jzqad_46GSDe"},"source":["input_=keras.layers.Input(shape=x_train.shape[1:])\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.Concatenate()([input_,hidden2])\n","output=keras.layers.Dense(1)(concat)\n","model=keras.Model(inputs=[input_],outputs=[output])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HNDTqNoiISB1"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wK9FcmBDLPpy"},"source":["input_A=keras.layers.Input(shape=[5],name=\"wide_input\")\n","input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.concatenate([input_A,hidden2])\n","output=keras.layers.Dense(1,name=\"output\")(concat)\n","model=keras.Model(inputs=[input_A,input_B],outputs=[output])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5sKw2ZGN5ex"},"source":["model.compile(loss=\"mse\",optimizer=keras.optimizers.SGD(lr=1e-3))\n","\n","x_train_a,x_train_b=x_train[:,:5],x_train[:,2:]\n","x_val_a,x_val_b=x_val[:,:5],x_val[:,2:]\n","x_test_a,x_test_b=x_test[:,:5],x_test[:,2:]\n","x_new_a,x_new_b=x_test_a[:3],x_test_b[:3]\n","\n","history=model.fit((x_train_a,x_train_b),y_train,epochs=10,batch_size=10,validation_data=((x_val_a,x_val_b),y_val))\n","mse_test=model.evaluate((x_test_a,x_test_b),y_test)\n","y_pred=model.predict((x_new_a,x_new_b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6v25tekYQ20Z"},"source":["* 여러개의 출력이 필요한 경우\n","  * 그림의 주요 물체를 분류하고 위치를 알아야 할 경우(회귀와 분류 둘다 사용)\n","  * 동일한 데이터에서 독립적인 여러 작업을 수행할 때, 얼굴 사진으로 다중 작업 분류(하나의 출력은 얼굴 표정, 다른 출력은 안경을 썼는지)\n","  * 규제 기법으로 사용하는 경우. 신경망 구조 안에 보조 출력을 추가할 수 있다."]},{"cell_type":"code","metadata":{"id":"nG_mx_AXQ3w0"},"source":["input_A=keras.layers.Input(shape=[5],name=\"wide_input\")\n","input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.concatenate([input_A,hidden2])\n","output=keras.layers.Dense(1,name=\"output\")(concat)\n","aux_output=keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n","model=keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRpBLOQVU1u9"},"source":["model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\") # 주 출력에 더 많은 가중치를 부여한다!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_RzdaEfCVHhU"},"source":["history=model.fit([x_train_a,x_train_b],[y_train,y_train],epochs=10,batch_size=5,validation_data=([x_val_a,x_val_b],[y_val,y_val]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxkpkA3gVacE"},"source":["total_loss,main_loss,aux_loss=model.evaluate([x_test_a,x_test_b],[y_test,y_test])\n","pd.DataFrame(history.history).plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROCS-SaVV_WT"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k8xGR4VDWR6z"},"source":["model.predict([x_new_a,x_new_b])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_fu9EQtWpHE"},"source":["model.evaluate([x_test_a,x_test_b],[y_test,y_test])  # 전체 손실, 주손실,보조손실"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DchvrIpEXMNu"},"source":["### 서브클래싱 API로 동적 모델 만들기\n","* 시퀀셜 API와 함수형 API는 모두 선언적이다. 층과 연결방식을 먼저 정의해야한다.\n","  * 모델의 구조를 출력-분석이 쉽고 에러를 일찍 발견하기에도 좋다.\n","* 정적이다는 것은 반복문과 조건문을 포함하고 사이즈를 변환하는 동적 구조를 포함하기에는 문제가 된다.\n","* 명령형 API를 위한 서브클래싱 API가 필수적이다.\n","* 함수형 API와 매우 비슷하지만 Input클래스의 객체를 만들 필요가 없다. 대신 call() 메서드의 input 매개변수를 사용한다.\n","* 주된 차이점은 call()메서드 안에서 원하는 어떤 계산도 사용할 수 있다는 것이다.\n","* for, if 텐서플로 저수준 연산을 사용할 수 있다.\n","* 모델 구조가 call메서드 안에 숨겨져 있기 때문에 케라스가 이를 쉽게 분석할 수 없다. 즉 모델을 저장하거나 복사할 수 없다.\n","* summary()메서드를 호출하면 층의 목록만 나열되고 층 간의 연결 정보를 얻을 수 없습니다. \n","* 케라스가 타입과 크기를 미리 확인할 수 없어 실수가 발생하기 쉽습니다."]},{"cell_type":"code","metadata":{"id":"PyxHt1VdW7rD"},"source":["class WideAndDeepModel(keras.models.Model):\n","    def __init__(self, units=30, activation=\"relu\", **kwargs):\n","        super().__init__(**kwargs)\n","        self.hidden1 = keras.layers.Dense(units, activation=activation)\n","        self.hidden2 = keras.layers.Dense(units, activation=activation)\n","        self.main_output = keras.layers.Dense(1)\n","        self.aux_output = keras.layers.Dense(1)\n","        \n","    def call(self, inputs):\n","        input_A, input_B = inputs\n","        hidden1 = self.hidden1(input_B)\n","        hidden2 = self.hidden2(hidden1)\n","        concat = keras.layers.concatenate([input_A, hidden2])\n","        main_output = self.main_output(concat)\n","        aux_output = self.aux_output(hidden2)\n","        return main_output, aux_output\n","\n","model = WideAndDeepModel(30, activation=\"relu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwJoWrNBjb9O"},"source":["model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")\n","history=model.fit([x_train_a,x_train_b],[y_train,y_train],epochs=10,validation_data=([x_val_a,x_val_b],[y_val,y_val]))\n","print(model.evaluate((x_test_a,x_test_b),(y_test,y_test)))\n","print(model.predict((x_new_a,x_new_b)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOfnRApep9Ft"},"source":["### 모델 저장과 복원\n","* 서브클래싱에서는 model.saver(\"~~\")를 사용할 수가 없다.\n","* save_weights() 와 load_weights() 메서드를 사용하여 모델 파라미터를 저장하고 복원할 수 있다.\n","* 그 외에는 모두 수동으로 저장하고 복원해야 한다."]},{"cell_type":"code","metadata":{"id":"13uNlzreu1SS"},"source":["housing=fetch_california_housing()\n","x_train_full,x_test,y_train_full,y_test=train_test_split(housing.data,housing.target)\n","x_train,x_val,y_train,y_val=train_test_split(x_train_full,y_train_full)\n","\n","scaler=StandardScaler()\n","x_train=scaler.fit_transform(x_train)\n","x_val=scaler.transform(x_val)\n","x_test=scaler.transform(x_test)\n","\n","x_train_a,x_train_b=x_train[:,:5],x_train[:,2:]\n","x_val_a,x_val_b=x_val[:,:5],x_val[:,2:]\n","x_test_a,x_test_b=x_test[:,:5],x_test[:,2:]\n","x_new_a,x_new_b=x_test_a[:3],x_test_b[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5rYm_t7nx2z"},"source":["input_A=keras.layers.Input(shape=[5],name=\"wide_input\")\n","input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.concatenate([input_A,hidden2])\n","output=keras.layers.Dense(1,name=\"output\")(concat)\n","aux_output=keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n","model=keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])\n","model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")\n","history=model.fit([x_train_a,x_train_b],[y_train,y_train],epochs=10,batch_size=5,validation_data=([x_val_a,x_val_b],[y_val,y_val]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6j1YVP6pFMC"},"source":["model.save(\"my_keras_model.h5\") # 저장"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fupHxFawpeF5"},"source":["model=keras.models.load_model(\"my_keras_model.h5\") # 불러오기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fojTNFbfpkns"},"source":["model.predict((x_new_a,x_new_b)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RylQAhcTpmte"},"source":["model.save_weights(\"my_keras_weigh.ckpt\") # 가중치 저장 서브클래스"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVCzb-AyrPtM"},"source":["model.load_weights(\"my_keras_weight.ckpt\") # 가중치 복원 서브클래스"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RYxUmoygrm2L"},"source":["### 콜벡\n","* 훈련이 몇시간동안 지속되면 훈련 도중 일정 간격으로 체크포인트를 저장해야한다.\n","* fit 메서드에서 이걸 콜백으로 할 수 있다.\n","\n","* ModelCheckpoint는 일정한 간격으로 모델의 체크포인트를 저장한다 기본적으로 매 에포크 끝에서 호출된다."]},{"cell_type":"code","metadata":{"id":"52rrGFavrYTB"},"source":["input_A=keras.layers.Input(shape=[5],name=\"wide_input\")\n","input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.concatenate([input_A,hidden2])\n","output=keras.layers.Dense(1,name=\"output\")(concat)\n","aux_output=keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n","model=keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])\n","model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTaWbwRjxESS"},"source":["#save_best_only=True 설정을 하면 최상의 검증 세트 점수에서만 모델을 저장한다.\n","# early_stopping_cb=keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)를 통해\n","# 일정 에포크 동안 검증세트에 대한 점수가 향상되지 않으면 훈련을 멈춘다.\n","# 모델이 향상되지 않으면 자동으로 중지되기 때문에 에포크의 숫자를 크게 지정해도 된다.\n","checkpoint_cb=keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",save_best_only=True)\n","early_stopping_cb=keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","history=model.fit([x_train_a,x_train_b],[y_train,y_train],epochs=10,batch_size=5,\n","                  validation_data=([x_val_a,x_val_b],[y_val,y_val]),callbacks=[checkpoint_cb,early_stopping_cb])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DF2RXBKAycTL"},"source":["* 사용자 정의 콜백\n","  * 아래의 콜백은 훈련하는 동안 검증 손실과 훈련 손실의 비율을 출력한다.\n","  * on_train_begin(),on_train_end(),on_epoch_begint(),on_epoch_end(),on_batch_begin(),on_batch_end() 등도 있다.\n","  * 평가에 사용하려면 on_test_begin()등으로 바꿀 수 있다.\n","  * 예측에 사용하려면 on_prdict_begin()등으로 바꿀 수 있다."]},{"cell_type":"code","metadata":{"id":"8QglAa-PyKhu"},"source":["class PrintValTrainRatioCallback(keras.callbacks.Callback):\n","  def on_epoch_end(self,epoch,logs):\n","    print(\"\\nval/train:{:.2f}\".format(logs[\"val_loss\"]/logs[\"loss\"]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAadLsr_0neQ"},"source":["### 텐서보드를 사용해 시각화하기\n","* 매우 좋은 인터렉티브 시각화 도구이다.\n","* 훈련하는 동안 학습 곡선을 그리거나 실행간의 학습 곡선을 비교하고 계산 그래프 시각화와 훈련 통계 분석을 수행할 수 있다.\n","* 모델이 생성한 이미지를 확인하거나 3D에 투영된 복잡한 다차원 데이터를 시각화 하고 자동으로 클러스터링을 해주는 등 많은 기능이 있다.\n","* 텐서보드를 사용하려면 프로그램을 수정하여 이벤트 파일이라는 특ㄱ별한 이진 로그 파일에 시각화하려는 데이터를 출력해야한다.\n","* 각각의 이진 데이터 레코드를 서머리라고 부른다.\n","* 일반적으로 텐서보드 서버가 루트로그 디렉터리를 가리키고 프로그램은 실행할 때마다 다른 서브디렉터리에 이벤트를 기록한다.\n","* 텐서보드 로그를 위해 사용할 루트 로그 디렉터리를 정의하고 현재 날짜와 시간을 사용해 실행할 때마다 다른 서브디렉터리 경로를 생성하는 간단한 함수를 만든다."]},{"cell_type":"code","metadata":{"id":"-ridRMJA1qFU"},"source":["import os\n","root_logdir = os.path.join(os.curdir,\"my_logs\")\n","\n","def get_run_logdir():\n","  import time\n","  run_id=time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n","  return os.path.join(root_logdir,run_id)\n","run_logdir=get_run_logdir()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbHnvJOt2Yya"},"source":["input_A=keras.layers.Input(shape=[5],name=\"wide_input\")\n","input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n","hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n","hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n","concat=keras.layers.concatenate([input_A,hidden2])\n","output=keras.layers.Dense(1,name=\"output\")(concat)\n","aux_output=keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n","model=keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])\n","model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")\n","# 모델 구성과 컴파일\n","tensorboard_cb=keras.callbacks.TensorBoard(run_logdir)\n","history=model.fit([x_train_a,x_train_b],[y_train,y_train],epochs=5,batch_size=20,\n","                  validation_data=([x_val_a,x_val_b],[y_val,y_val]),callbacks=[tensorboard_cb])\n","# 이를 실행하면 로그 디렉터리를 생성한다. 훈련하는 동안 이벤트 파일을 만들고 서머리를 기록한다.\n","# 실행때마다 하나으 ㅣ디렉터리가 생성된다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9_b_7PL471z"},"source":["%load_ext tensorboard\n","%tensorboard --logdir=./my_logs --port=6006\n","# 주피터 안에서 바로 텐서보드를 실행할 수 있다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"voPbtEQR64Ca"},"source":["### 신경망 하이퍼파라미터 튜닝하기\n","* 많은하이퍼 파라미터를 시도하고 교차검증으로 가장 좋은 점수를 확인하는것!\n","* GridSearchCv나 랜덤서치로 하이퍼 파라미터 공간 탐색\n","* 케라스 모델을 사이킷런 추정기처럼 보이도록 바꾸어야 한다.\n"]},{"cell_type":"code","metadata":{"id":"aNfhbDNf8AYw"},"source":["def build_model(n_hidden=1,n_neurons=30,learning_rate=3e-3,input_shape=[8]):\n","  model=keras.models.Sequential()\n","  model.add(keras.layers.InputLayer(input_shape=input_shape))\n","  for layer in range(n_hidden):\n","    model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))\n","  model.add(keras.layers.Dense(1))\n","  optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n","  model.compile(loss=\"mse\",optimizer=optimizer)\n","  return model\n","# 사이킷런과 마찬가지로 가능한 하이퍼파라미터에 적절한 기본값을 설정하는 것이 좋다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nGlQzieF8pPp"},"source":["# buildmodel함수를 사용해 kerasRegressor 클래스의 객체를 만든다.\n","keras_reg=keras.wrappers.scikit_learn.KerasRegressor(build_model)\n","#이제 일반적인 사이킷런 회귀 추정기처럼 이 객체를 사용할 수 있다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K_sTqnLo9T0I"},"source":["from scipy.stats import reciprocal\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","param_distribs={\n","    \"n_hidden\":[0,1,2,3],\n","    \"n_neurons\":np.arange(1,100),\n","    \"learning_rate\":reciprocal(3e-4,3e-2),\n","}\n","\n","rnd_search_cv=RandomizedSearchCV(keras_reg,param_distribs,n_iter=10,cv=3)\n","rnd_search_cv.fit(x_train,y_train,epochs=20,\n","                  validation_data=(x_val,y_val),\n","                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3wWOE0f-Scw"},"source":["rnd_search_cv.best_params_,rnd_search_cv.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hzqtf7N_Bjt"},"source":["model=rnd_search_cv.best_estimator_.model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xISyhoB6_aM5"},"source":["* 페이지 400쪽 굉장히 많은 하이퍼파라미터 튜닝 라이브러리가 있다.\n","---\n","### 은닉층 개수\n","* 복잡한 문제에서는 심층신경망이 파라미터 효율성이 좋다.\n","* 계층구조는 좋은 솔루션 뿐만 아니라 새로운 데이터에 일반화 되는 능력도 향상시켜준다\n","  * 얼굴을 인식하는 모델에서 헤어스타일을 인식하게 하려면 첫번째 네트워크의 하위층을 재사용하여 훈련을 시작할 수 있다.\n","  * 그 층들의 가중치와 편향값으로 초기화하면 저수준 구조를 학습할 필요가 없게된다. 이를 전이학습이라한다.\n","  * 네트워크는 비슷한 작업에서 뛰어난 성능을 낸 훈련된 네트워크의 일부를 재사용하는게 일반적이다.\n","---"]},{"cell_type":"markdown","metadata":{"id":"q9s8l92CBvvR"},"source":["### 은닉층의 뉴런 개수\n","* 데이터셋에 따라 다르지만 다른 은닉층보다 첫번째 은닉층을 크게하는게 도움이 된다.\n","* 과대적합이 시작되기 전까지 점진적으로 뉴런 수를 늘릴 수 있다.\n","  * 실전에서는 필요한 것보다 더 많은 층과 뉴런을 가진 모델을 선택하고, 그 다음 과대적합되지 않도록 조기종료나 규제기법을 사용하는것이 간단하고 효과적이다.\n","  * 뉴런의 수가 너무 적으면 유용한 정보를 표현할 충분한 표현 능력을 가지지 못한다.\n","---"]},{"cell_type":"markdown","metadata":{"id":"uAdW9BiDCjkS"},"source":["### 학습률,배치크기 그리고 다른 하이퍼 파라미터\n","##### 학습률\n","* 일반적으로 최적의 학습률은 최대학습률(훈련 알고리즘이 발산하는)의 절반정도이다.\n","* 좋은 학습률을 찾는 한가지 방법은 매우 낮은 학습률에서 매우 큰 학습률까지 수백번 반복하여 모델을 훈련하는 것이다.\n","* 반복마다 일정한 값을 학습률에 곱한다.\n","* 최적의 학습률은 손실이 다시 상승하는 점보다 조금 아래에 있을 것이다. 일반적으로 상승점보다 10배 낮은 지점.\n","##### 옵티마이저\n","* 좋은 옵티마이저를 선택하는것과 그것의 튜닝이 매우 중요!\n","##### 배치 크기\n","* 큰 배치 크기를 사용하면 GPU와 같은 하드웨어 가속기를 효율적으로 활용할 수 있다는 장점!\n","* GPU 램에 맞는 가장 큰 배치 크기를 사용하는 것이 좋다!.\n","* 큰 배치 크기는 일반화 성능에 영향을 미치지 않고 훈련 시간을 매우 단축한다.\n","  * 따라서 학습률 예열(작은 학습률 보다 커지게)같은 기법으로 매우 큰 배치 크기를 시도해 보는 것이 좋다. 성능이 만족하지 못하면 작은 배치 크기를 사용해보자!\n","##### 활성화 함수\n","* 일반적으로 Relu가 은닉층에 좋은 기본값이다. 출력은 작업에 따라 다르다.\n","##### 반복횟수\n","* 보통 튜닝할 필요가 없고 조기종료를 사용한다\n","  * 레슬리 스미스의 2018년 논문 확인\n","  "]},{"cell_type":"code","metadata":{"id":"X7kT3MSP_I3P"},"source":[""],"execution_count":null,"outputs":[]}]}