{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-한눈에 보는 머신러닝","provenance":[],"authorship_tag":"ABX9TyP8COlIpmRSwPowQ0Aussgj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9voMtTY193Kr"},"source":["## 1.지도학습\n","  * 레이블을 가짐\n","  * 분류와 회귀\n","  * 알고리즘\n","    * knn,선형 회귀,로지스틱 회귀, 서포트 벡터 머신\n","    * 결정트리 와 랜덤포레스트, 신경망\n","\n","## 2.비지도학습\n","  * 레이블이 없다.\n","  * 군집\n","    * k-mean, DBSCAN, 계층 군집 분석. 이상치 탐지 와 특이치 탐지, 원 클래스, 아이솔레이션 포레스트\n","  * 시각화\n","    * 레이블이 없는 고차원 데이터로 2D나 3D표현을 만듬\n","  * 차원축소\n","    * 상관관계가 있는 여러 특성을 하나로 합침(특성추출)\n","  * 이상치 탐지\n","    * 새로운 셈플이 이상치인지 판단\n","  * 특이치 탐지\n","    * 훈련 세트에 있는 모든 샘플과 달라보이느 새로운 샘플 탐지\n","    * 감지하고자 하는 샘플을 제거한 깨끗한 훈련세트 필요\n","  * 연관 규칙 학습\n","    * 데이터 특성 간의 관계 탐지 ex)바비큐 소스와 감자를 구매한 사람이 스테이크도 구매하는 등\n","\n","## 3.준지도 학습\n","  * 일부만 레이블이 있는 학습 데이터 다룸\n","  * 비지도학습 방식으로 훈련된 다음 지도 학스 방식으로 세밀하게 조정\n","\n","## 4.강화 학습\n","  * 에이전트가 환경을 관찰해 행동하고 결과로 보상 또는 벌점을 받아, 스스로 정책을 수립한다.\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"PLGuXoulKpyh"},"source":["# 머신러닝 프로젝트 순서\n","  1. 데이터를 분석\n","  2. 모델은 선택\n","  3. 훈련 데이터로 모델을 훈련\n","  4. 새로운 데이터에 모델을 적용해 예측하고 평가\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"40RuQF1yVoit"},"source":["## 대표성 없는 훈련데이터\n","  * 일반화를 위해 훈련데이터의 대표성이 중요\n","  * 셈플이 작을경우 <u/>셈플링 잡음(우연에 의한 대표성 없는 데이터)</u>\n","  * 표본추출이 잘못 될 경우 대표성을 띄지 못하는 <u/>셈플링 편향</u>\n","----"]},{"cell_type":"markdown","metadata":{"id":"fqY_Lm5TWZqR"},"source":["## 낮은 품질의 데이터\n","  * 일부 샘플이 이상치라는 게 명확하면 무시하거나 수동으로 수정\n","  * 특성 몇개가 빠져있다면 무시할지 빠진 값을 채울지, 넣은 모델과 뺀 모델을 따로 훈련할지 결정\n","  ---\n"]},{"cell_type":"markdown","metadata":{"id":"Fo35g5e1XP1j"},"source":["# 관련없는 특성\n","  * 관련없는 특성을 줄이고 좋은 특성을 찾는것\n","    * 이 과정을 특성공학이라 한다.\n","      * 특성 선택 : 특성중 훈련에 유용한 특징 선택\n","      * 특성 추출 : 특성을 결합하여 유용한 특성 만듬 ex) 차원 축소 알고리즘\n","      * 새로운 데이터를 수집해 새 특성을 만듬\n","---"]},{"cell_type":"markdown","metadata":{"id":"3xi5ekL9YKeT"},"source":["### 과대적합\n","  * ex) 해외여행중 소매치기를 당하면 소매치기가 많은 나라라고 생각하는 것\n","  * 훈련데이터에 너무 잘 맞지만 일반성이 떨어진다.\n","  * 과대적합을 감소시키기 위한 제약을 규제라고한다.\n","---\n","\n","#### *  하이퍼파라미터: 모델의 파라미터값\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"8piTD1NrbJtg"},"source":["### 과소적합\n","  * 모델이 너무 단순해서 내재된 구조를 학습하지 못함.\n","    * 더 강력한 모델 선택\n","    * 모델의 제약을 줄인다.\n","----"]},{"cell_type":"markdown","metadata":{"id":"6ovrYgN6cEPv"},"source":["### 홀드아웃 검증\n","  * 훈련세트의 일부를 떼어내어 모델을 평가하고 가장 좋은 하나를 선택한다.\n","-----"]},{"cell_type":"markdown","metadata":{"id":"0Q1TutFacmtl"},"source":["### 교차검증\n","  * 검증세트마다 나머지 데이터에서 훈련한 모델을 해당 검증세트에서 평가한다.\n","---"]},{"cell_type":"code","metadata":{"id":"aDBVrCt-bH8D"},"source":[""],"execution_count":null,"outputs":[]}]}