{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"14-합성곱 신경망을 사용한 컴퓨터 비전.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMhBmCdCJpXpFaVqJgeIsVm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aOSQtGNPMxoo"},"source":["# 합성곱 신경망을 사용한 컴퓨터 비전\n","* CNN은 대뇌의의 시각 피질 연구에서 시작되고 이미지 인식에 사용됨.\n","### 합성곱 층\n","* 합성곱 층의 뉴런은 입력 이미지의 모든 픽셀에 연결되는 것이 아니라 수용장 안에 있는 픽셀에만 연결된다.\n","* 두번째 층의 뉴런은 첫번째 층의 작운 사각영역 안에 위치한 뉴런에 연결된다.\n","* 네트워크가 첫번째 은닉층에서는 작은 저수준 특성에 집중하고 그 다음 은닉층에서는 더 고수준 특성으로 조합해 나간다.\n","* 스트라이드가 있다!.\n","### 필터\n","* 뉴런의 가중치는 수용장 크기의 작은 이미지로 표현될 수있다.\n","  * 필터(합성곱 커널)\n","* 층에 적용된 하나의 필터는 하나의 특성 맵을 만든다.\n","---\n","### 여러가지 특성 맵 쌓기\n","* 실제 합성곱 층은 여러가지 필터를 가지고 필터마다 하나의 특성 맵을 출력하므로 3D로 표현하는 것이 더 정확하다.\n","* 하나으 피쳐맵의 뉴런은 같은 파라미터를 공유하고 다른 피쳐맵의 뉴런은 다른 파라미터를 사용한다.\n","* 합성곱 층이 입력에 여러 필터를 동시에 적용하여 입력의 여러 특성을 감지 가능.\n","* 입력이미지는 컬러채널로 여러 서브층으로 구성되기도 한다. RGB\n","* 합성곱 l층의 뉴런은 이전층의 해당 위치의 특성맵으로 연결되어있다.\n","---\n","### 텐서플로 구현\n","* 텐서플로에서 각 입력이미지는 보통 [높이,너비,채널]의 3D텐서로 구성된다.\n","* 하나의 미니배치는 [미니배치 크기,높이,너비,채널] 형태의 4D텐서로 표현된다.\n","* 합성곱 층의 가중치는 [fh,fw,fn,fn]의 4D텐서로 표현된다. 편향은[fn]텐서\n","* load_sample_images() 이미지 받고 사용하기 위해 Pillow사용"]},{"cell_type":"code","metadata":{"id":"-t52-Cj7Upl2"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpqDX-spMt5N"},"source":["from sklearn.datasets import load_sample_image\n","china=load_sample_image(\"china.jpg\")/255 # 각 컬러의 픽셀 강도는 0~255사이의 크기 이를 255로 나눠 0~1로 바꿈\n","flower=load_sample_image(\"flower.jpg\")/255 #\n","images=np.array([china,flower])\n","batch_size,height,wedth,channels=images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGqI1g_-fNiv"},"source":["def crop(images):\n","  return images[150:220,130:250]\n","def plot_image(image):\n","    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n","    plt.axis(\"off\")\n","\n","def plot_color_image(image):\n","    plt.imshow(image, interpolation=\"nearest\")\n","    plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQOn3xI0VJPM"},"source":["filters=np.zeros(shape=(7,7,channels,2),dtype=np.float32) #7*7 필터 만듬\n","filters[:,3,:,0]=1 #가운데 흰 수직선\n","filters[3,:,:,1]=1 # 가운데 수평선\n","\n","output=tf.nn.conv2d(images,filters,strides=1,padding=\"SAME\")# 제로패딩과 스트라이드 1\n","# images는 4D텐서, filters는 4D텐서, padding= \"VALID\"의 경우 제로패딩 안씀.\n","plt.imshow(output[1,:,:,0],cmap=\"gray\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW9p7vd9fYxe"},"source":["plot_image(crop(images[0, :, :, 0]))\n","plt.show()\n","\n","for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n","    plot_image(crop(output[0, :, :, feature_map_index]))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYg7D27TZbQO"},"source":["* 신경망 conv층\n","  * 3*3사이즈의 32개 필터와 1스트라이드 세임패딩,렐루함수"]},{"cell_type":"code","metadata":{"id":"AQepjaZ8WI6U"},"source":["conv=tf.keras.layers.Conv2D(filters=32,kernel_size=3,strides=1,\n","                         padding=\"same\",activation=\"relu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpjwpnMdZ2sl"},"source":["### 메모리 요구 사항\n","* CNN은 많은 램을 사용한다.\n","* 5*5필터로 스트라이드1 \"same\"패딩을 사용해 150*100크기의 특성맵 200개의 합성곱층 생각하자\n","  * 파라미터 수는(5*5*3+1)*200=15200이다.\n","  * 200개의 피쳐맵 마다 150*100개의 뉴런을 포함하고 각 뉴런은 5*5*3개의 입력에 대한 가중치 합을 계산해야한다.\n","  * 즉 2억2500만개의 실수 곱샘이 있다.\n","  *32비트 부동소수로 표현되면 출력이 ram의 200*150*100*32=9천600만 비트(12MB)를 소모한다. 이는 한 샘플에 대한 것이다.\n","  * 훈련 배치가 100개의 샘플로 이루어져 있다면 이는 1.2GB의 램을 사용할 것이다.\n","  * 추론을 위해서는 하나의 층이 점유하는 램은 다음층에서 해소된다.\n","  * 그러나 훈련시에는 역방향을 위해 모두 보전되어야 한다.\n","  * 메모리 부족으로 훈련이 실패한다면 미니배치 크기를 줄이거나 스트라이드를 사용해 차원을 줄이거나 층을 제거할 수 있다. 32비트 대신 16비트 부동소수도 가능.\n","  * 여러 장치에 CNN 분산도 가능.\n","---\n","### 풀링 층\n","* 계산량과 메모리 사용량,파라미터수 줄임(과대적합 해소)를 위한 부표본 생성\n","* 동일하게 크기,스트라이드,패딩을 지정하지만 가중치가 없다.\n","* 최대나 평균같은 합산 함수 사용이 전부이다.\n","* 2*2커널 2 스트라이드를 사용하면 이밎의 높이와 너비는 각 절반이된다.\n","* 작은 변화에도 일정 수준의 불변성을 만든다. 이동 불변성 가짐.\n","---\n","### 텐서플로 구현\n","*  MaxPool2D,AvgPool2D\n","* 멕스풀링이 더 성능이 좋다. 정보손실은 더 있지만 큰 특징 유지한다."]},{"cell_type":"code","metadata":{"id":"2C-aT616ZnGr"},"source":["max_pool=tf.keras.layers.MaxPool2D(pool_size=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cwifhl2sfCnc"},"source":["cropped_images = np.array([crop(image) for image in images], dtype=np.float32)\n","output = max_pool(cropped_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDe1QbNrf96U"},"source":["plt.figure(figsize=(12, 8))\n","plt.subplot(121)\n","plt.imshow(cropped_images[0])\n","plt.subplot(122)\n","plt.imshow(output[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzdUceBKheqR"},"source":["* 케라스는 깊이방향 풀링을 제공하지 않지만 만들 수 있다.\n","* 깊이방향 풀링은 회전층에 상관 없이 동일 출력을 만든다."]},{"cell_type":"code","metadata":{"id":"Rms701A6hsVz"},"source":["class DepthMaxPool(tf.keras.layers.Layer):\n","    def __init__(self, pool_size, strides=None, padding=\"VALID\", **kwargs):\n","        super().__init__(**kwargs)\n","        if strides is None:\n","            strides = pool_size\n","        self.pool_size = pool_size\n","        self.strides = strides\n","        self.padding = padding\n","    def call(self, inputs):\n","        return tf.nn.max_pool(inputs,\n","                              ksize=(1, 1, 1, self.pool_size),\n","                              strides=(1, 1, 1, self.pool_size),\n","                              padding=self.padding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mI469x52iDj8"},"source":["depth_pool = DepthMaxPool(3)\n","with tf.device(\"/cpu:0\"): # there is no GPU-kernel yet\n","    depth_output = depth_pool(cropped_images)\n","depth_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mi2cwxj6i48U"},"source":["# 케라스 모델의 층으로 사용하고싶을때 사용자 정의(위에) 또는 람다 방식으로!.\n","depth_pool = tf.keras.layers.Lambda(lambda X: tf.nn.max_pool(\n","    X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"VALID\"))\n","with tf.device(\"/cpu:0\"): # there is no GPU-kernel yet\n","    depth_output = depth_pool(cropped_images)\n","depth_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-j6a3TjjG2X"},"source":["* 전역 평균 풀링 층!\n","  * 각 특성 맵의 평균을 계산한다.\n","  * 특성 맵 마다 하나의 출력을 뽑는다.(대부분의 정보 손실)\n","  * 매우 파괴적이지만 출력층에는 유용하다."]},{"cell_type":"code","metadata":{"id":"B_VSiMoUjEel"},"source":["\n","global_avg_pool=tf.keras.layers.GlobalAveragePooling2D()\n","global_avg_pool(cropped_images)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cH5qpbR3jqZ1"},"source":["output_global_avg2=tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=[1,2]))\n","output_global_avg2(cropped_images)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnfwUnCAj8ig"},"source":["### CNN구조\n","* 전형적인 CNN구조는 합성곱 층을 몇 개 쌓고, 그 다음에 풀링층을 쌓고 또 합성곱 층을 쌓고 풀링쌓고 이런식이다.\n","* 네트워크를 통과할수록 이미지는 작아지지만 일반적으로 합성곱 층 때문에 점점 깊어진다.\n","* 합성곱 층에 너무 큰 커널을 사용하는것은 흔한 실수이다.\n","  * 5*5대신 3*3 두개를 쌓는게 계산량이 적고 더 나은 성능을 낸다.\n","  * 예외로 첫번째 합성곱 층은 큰 크기의 커널과 2이상의 스트라이드를 사용한다. 정보를 너무 잃지 않고 공간 방향 차원을 줄일 수 있다.\n","  * CNN 출력층에 다다를수록 필터 개수가 늘어난다. 저수준 특성을 연결해서 고수준 특성을 만들기에 합리저.\n","  * 필터 개수를 두배로 늘리는 것이 일반적이다."]},{"cell_type":"code","metadata":{"id":"LiV3M9gzj5Hd"},"source":["from tensorflow import keras\n","(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n","y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n","\n","X_mean = X_train.mean(axis=0, keepdims=True)\n","X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n","X_train = (X_train - X_mean) / X_std\n","X_valid = (X_valid - X_mean) / X_std\n","X_test = (X_test - X_mean) / X_std\n","\n","X_train = X_train[..., np.newaxis]\n","X_valid = X_valid[..., np.newaxis]\n","X_test = X_test[..., np.newaxis]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxVE7VofmvQd"},"source":["from functools import partial\n","\n","DefaultConv2D = partial(keras.layers.Conv2D,\n","                        kernel_size=3, activation='relu', padding=\"SAME\")\n","model=keras.models.Sequential([\n","                               DefaultConv2D(filters=64,kernel_size=7,input_shape=[28,28,1]),\n","                               keras.layers.MaxPooling2D(pool_size=2),\n","                               DefaultConv2D(filters=128),\n","                               DefaultConv2D(filters=128),\n","                               keras.layers.MaxPooling2D(pool_size=2),\n","                               DefaultConv2D(filters=256),\n","                               DefaultConv2D(filters=256),\n","                               keras.layers.Flatten(),\n","                               keras.layers.Dense(128,activation=\"relu\"),\n","                               keras.layers.Dropout(0.5),\n","                               keras.layers.Dense(64,activation=\"relu\"),\n","                               keras.layers.Dropout(0.5),\n","                               keras.layers.Dense(10,activation=\"softmax\")\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bl5xznnPojUc"},"source":["model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",metrics=[\"accuracy\"])\n","model.fit(X_train,y_train,epochs=5,validation_data=(X_valid,y_valid),batch_size=20)\n","score=model.evaluate(X_test,y_test)\n","X_new=X_test[:10]\n","model.predict(X_new),score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCEVs7tFsJEZ"},"source":["### LeNet-5\n","* 1998 르쿤, MNist에 사용"]},{"cell_type":"code","metadata":{"id":"RIWzMpyIrR43"},"source":["model=keras.models.Sequential([\n","                               keras.layers.Conv2D(filters=6,kernel_size=5,strides=1,activation=\"tanh\",input_shape=[28,28,1]),\n","                               keras.layers.AveragePooling2D(pool_size=2,strides=2),\n","                               keras.layers.Conv2D(filters=16,kernel_size=3,strides=1,activation=\"tanh\"),\n","                               keras.layers.AveragePooling2D(pool_size=2,strides=2),\n","                               keras.layers.Conv2D(filters=120,kernel_size=5,strides=1,activation=\"tanh\"),\n","                               keras.layers.Flatten(),\n","                               keras.layers.Dense(84,activation=\"softmax\")\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyrzrkJ0tEo-"},"source":["model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n","model.fit(X_train,y_train,batch_size=20,epochs=5,validation_data=(X_valid,y_valid))\n","model.evaluate(X_test,y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nncY6v7wf3Q"},"source":["### AlexNet\n","* 2012 이미지넷 대회 성능 매우 향승\n","* 드롭아웃적용.\n","* 이미지를 랜덤하게 이동하거나 수평으로 뒤집고 조명을 바꾸는 등 데이터 증식."]},{"cell_type":"code","metadata":{"id":"cSWG5oK0vE7d"},"source":["model=keras.models.Sequential([\n","                               keras.layers.Conv2D(filters=96,kernel_size=11,strides=4,activation=\"relu\",input_shape=[227,227,3]),\n","                               keras.layers.MaxPooling2D(pool_size=3,strides=2),\n","                               keras.layers.Conv2D(filters=256,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"),\n","                               keras.layers.MaxPooling2D(pool_size=3,strides=2),\n","                               keras.layers.Conv2D(filters=384,kernel_size=3,strides=1,padding=\"same\",activation=\"relu\"),\n","                               keras.layers.Conv2D(filters=384,kernel_size=3,strides=1,padding=\"same\",activation=\"relu\"),\n","                               keras.layers.Conv2D(filters=384,kernel_size=3,strides=1,padding=\"same\",activation=\"relu\"),\n","                               keras.layers.MaxPooling2D(pool_size=3,strides=2),\n","                               keras.layers.Dense(4096,activation=\"relu\"),\n","                               keras.layers.Dropout(rate=0.5),\n","                               keras.layers.Dense(4096,activation=\"relu\"),\n","                               keras.layers.Dense(1000,activation=\"softmax\")\n","\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBJzTGhNMI_Y"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhRDFuNoMiX5"},"source":["### GoogleNet\n","* GoogleNet은 Alexnet보다 10배 적은 파라미터를 가진다.\n","* 1*1,3*3,5*5층 등을 사용해서 각 연산후 concat해서 하나의 출력을 만든다.\n","----\n","### VGGNET\n","* 2014년 2등,\n","---\n","### ResNet\n","* 잔차 네트워크를 사용해 2015 우승.\n","* 152개 층의 매우 깊은 CNN\n","* 적은 파라미터로 깊은 네트워크 훈련하는 트렌드 만듬.\n","* 스킵연결,숏컷 연결. 어떤층에 주입되는 신호가 상위 층의 출력에 더해진다.\n","* 출력에 x(입력)을 더하면 h(x) 데신 f(x)=h(x)-x로 학습할 것이다. 이를 잔차학습이라 한다.\n","* 신경망 초기화시 가중치가 0에 가깝기에 네트워크도 0에 가까운 값을 출력한다.\n","  * 스킵 연결을 추가하면 이 네트워크는 입력과 같은 값을 출력한다.\n","  * 초기에 항등 함수를 모델링한다. 목적함수가 항등함수에 가깝기에 훈련속도가 매우 빠르다.\n","* 스킵 연결 덕에 입력 신호가 전체 네트워크에 손쉽게 영향을 끼친다.\n","* 잔차 유닛(RU)=Relu,3*3커널,배치정규화 사용.\n","* 잔차 유닛을 매우 깊게 쌓고 배치정규화함\n","  * 특성의 크기가 바뀌는 잔차유닛에서는 스트라이드 2이고 출력 특성매의 수가 같은 1*1 합성곱 층으로 입력을 통과시긴다.\n","* ResNet-34는 34는 34개 층으로 이루어진 ResNet으로 64개의 피쳐맵 출력하는 3개 RU, 128개 맵의 4개 RU,256개 맵의 6개RU, 원본 깊이를 복원하는 256개 피쳐맵의 1*1 합성곱층으로 이루어짐.\n","---\n","### Xception\n","* 2016 Google넷의 변종\n","* googlenet과 resnet의 아이디어를 합친다.\n","* 공간 패턴과 채널 사이 패턴을 분리하여 모델링 한다.\n","---\n","###SeNet\n","* 2017 우승\n","* 인셉션 모듈이나 잔차유닛에 SE블록이라는 신경망 추가.\n","* 유닛의 깊이 차원에 초점을 맞추어 분석한다. 동시에 활성화되는 특성을 학습.\n","* 덜 활성화 된다면 출력을 높이는 방식.\n","* SE층은 3개의 층으로 구성, 전역 평균 풀링층과, RElu사용 밀집 은닉층, Sigmoid 사용 밀집 출력층이다.\n","---\n","### 케라스를 사용해 ResNet-34 CNN 구현하기!"]},{"cell_type":"code","metadata":{"id":"sYWK50-9MdBg"},"source":["class ResidualUnit(keras.layers.Layer):\n","  def __init__(self,filters,strides=1,activation=\"relu\",**kwargs):\n","    super().__init__(**kwargs)\n","    self.activation=keras.activations.get(activation)\n","    self.main_layers=[\n","                      keras.layers.Conv2D(filters,3,strides=strides,\n","                      padding=\"same\",use_bias=False),\n","                      keras.layers.BatchNormalization(),\n","                      self.activation,\n","                      keras.layers.Conv2D(filters,3,strides=1,padding=\"same\",use_bias=False),\n","                      keras.layers.BatchNormalization()\n","    ]\n","    self.skip_layers=[]\n","    if strides>1:\n","      self.skip_layers=[\n","                        keras.layers.Conv2D(filters,1,strides=strides,padding=\"same\",use_bias=False),\n","                        keras.layers.BatchNormalization()\n","      ]\n","    def call(self,inputs):\n","      Z=inputs\n","      for layer in self.main_layers:\n","        Z=layer(Z)\n","      skip_z=inputs\n","      for layer in self.skip_layers:\n","        skip_z=layer(skip_z)\n","      return self.activation(Z+skip_z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5e5-gblHW9ZR"},"source":["model=keras.models.Sequential()\n","model.add(keras.layers.Conv2D(64,7,strides=2,input_shape=[224,224,3],\n","                              padding=\"same\",use_bias=False))\n","model.add(keras.layers.BatchNormalization())\n","model.add(keras.layers.Activation(\"relu\"))\n","model.add(keras.layers.MaxPooling2D(pool_size=3,strides=2,padding=\"same\"))\n","prev_filters=1\n","for filters in [64]*3+[128]*4+[256]*6+[512]*3:\n","  strides =1 if filters==prev_filters else 2\n","  model.add(ResidualUnit(filters,strides=strides))\n","  prev_filters=filters\n","model.add(keras.layers.GlobalAvgPool2D())\n","model.add(keras.layers.Flatten())\n","model.add(keras.layers.Dense(10,activation=\"softmax\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kryv0KoDazFC"},"source":["### 케라스에서 제공하는 사전훈련된 모델 사용하기\n","* keras.applications 패키지에 준비된 사전 훈련된 모델을 코드로 불러온다~!"]},{"cell_type":"code","metadata":{"id":"Kzb3QoCfamuh"},"source":["model=keras.applications.resnet50.ResNet50(weights=\"imagenet\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRaM6LawbbI5"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jfxdG6ojb1Ux"},"source":["images_resized=tf.image.resize(images,[224,224])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mh837tP8cVP9"},"source":["* 사전훈련된 모델은 이미지가 적절하게 전처리 되었다고 가정한다.\n","* 따라서 맞춰서 저장해야한다."]},{"cell_type":"code","metadata":{"id":"XJ1CkM2bcHCQ"},"source":["inputs=keras.applications.resnet50.preprocess_input(images_resized*255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYPWjvDlcgrx"},"source":["y_proba=model.predict(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TGk6RhprckaB"},"source":["y_proba # 행이 하나의 이미지이고 열이 클래스인 행렬이다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSznaKG6c6rs"},"source":["* 최상위 K개의 예측을 클래스 이름과 추정 황률과 출력하려면 decode_predictions()함수를 사용한다.\n","* 사전훈련된 모델을 사용해 좋은 이미지 분류기를 만드는 것은 아주 쉽다.\n","* 다른종류의 모델 ResNet,Inception-v3,Xception,vgg등 여러 모델이 있다."]},{"cell_type":"code","metadata":{"id":"v48qJmQgclTx"},"source":["top_k=keras.applications.resnet50.decode_predictions(y_proba,top=3)\n","for image_index in range(len(images)):\n","  print(\"이미지 #{}\".format(image_index))\n","  for class_id,name,y_proba in top_k[image_index]:\n","    print(\" {} - {:12s} {:.2f}%\".format(class_id,name,y_proba*100))\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Xp87l8-fM3c"},"source":["### 사전훈련된 모델을 사용한 전이 학습.\n","* 충분하지 않은 훈련데이터로 학습하려면 사전훈련 모델의 하위층을 사용하는 것이 좋다."]},{"cell_type":"code","metadata":{"id":"BK4tN7c2eC26"},"source":["import tensorflow_datasets as tfds\n","dataset,info=tfds.load(\"tf_flowers\",as_supervised=True,with_info=True)# with_info사용하면 데이터셋에 대한 정보 얻을수있다.\n","dataset_size=info.splits[\"train\"].num_examples #3670\n","class_names=info.features[\"label\"].names \n","n_classes=info.features[\"label\"].num_classes # 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rpE2t5dIhRGU"},"source":["* train세트만 있고 테스트나 검증세트는 없다.\n","* 훈련세트를 나누어야한다. TF데이터셋에는 이를 위한 API가 제공된다."]},{"cell_type":"code","metadata":{"id":"dYehzupskTK6"},"source":["test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n","    \"tf_flowers\",\n","    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n","    as_supervised=True)\n","#나누기!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wA1a3EcglFjZ"},"source":["plt.figure(figsize=(12, 10))\n","index = 0\n","for image, label in train_set_raw.take(9):\n","    index += 1\n","    plt.subplot(3, 3, index)\n","    plt.imshow(image)\n","    plt.title(\"Class: {}\".format(class_names[label]))\n","    plt.axis(\"off\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qoz7ytCLoJps"},"source":["* 데이터 전처리!"]},{"cell_type":"code","metadata":{"id":"WdCBt3gSlePh"},"source":["def preprocess(image,label):\n","  resized_image=tf.image.resize(image,[224,224])\n","  final_image=keras.applications.xception.preprocess_input(resized_image)\n","  return final_image,label\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjBfoNqlmFxS"},"source":["\n","def central_crop(image):\n","    shape = tf.shape(image)\n","    min_dim = tf.reduce_min([shape[0], shape[1]])\n","    top_crop = (shape[0] - min_dim) // 4\n","    bottom_crop = shape[0] - top_crop\n","    left_crop = (shape[1] - min_dim) // 4\n","    right_crop = shape[1] - left_crop\n","    return image[top_crop:bottom_crop, left_crop:right_crop]\n","\n","def random_crop(image):\n","    shape = tf.shape(image)\n","    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n","    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n","\n","def preprocess(image, label, randomize=False):\n","    if randomize:\n","        cropped_image = random_crop(image)\n","        cropped_image = tf.image.random_flip_left_right(cropped_image)\n","    else:\n","        cropped_image = central_crop(image)\n","    resized_image = tf.image.resize(cropped_image, [224, 224])\n","    final_image = keras.applications.xception.preprocess_input(resized_image)\n","    return final_image, label\n","\n","batch_size = 32\n","train_set = train_set_raw.shuffle(1000)\n","train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n","valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n","test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ODCAxyX-oNcV"},"source":["* keras.preprocessing.image.ImageataGenerator클래스를 사용하면 손쉽게 디스크에서 이미지를 적재하여 여러 방식으로 증식이 가능하다.\n","---\n","* 사전훈련된 Xception 모델 로드\n","  * include_top=False로 최상층의 전역 평균 풀링 층과 밀집 출력 층 제외시킨다.\n","  * 기반모델의 새로운 전역 평균 풀링 층을 추가하고 그 뒤에 클래스마다 하나의 유닛과 소프트맥스 활성화 함수를 가진 밀집 출력 층을 놓는다."]},{"cell_type":"code","metadata":{"id":"4BLle01ln036"},"source":["base_model=keras.applications.xception.Xception(weights=\"imagenet\",include_top=False)\n","avg=keras.layers.GlobalAveragePooling2D()(base_model.output)\n","output=keras.layers.Dense(n_classes,activation=\"softmax\")(avg)\n","model=keras.Model(inputs=base_model.input,outputs=output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST8xAMpXpIQ5"},"source":["for layer in base_model.layers:\n","  layer.trainable=False\n","# 훈련 초기에는 사전훈련된 층의 가중치를 동결한다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVXK-adDpWDB"},"source":["optimizer=keras.optimizers.SGD(lr=0.2,momentum=0.9,decay=0.01)\n","model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n","history=model.fit(train_set,epochs=5,validation_data=valid_set,batch_size=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmUT4qZvqxti"},"source":["### 분류와 위치 추정\n","* 사진에서 물체 위치 추정은 회귀 작업이다.\n","* 바운딩박스는 수평,수직좌표와 높이,너비를 예측하는 것이다.\n"]},{"cell_type":"code","metadata":{"id":"ZzLakHhxr4Zg"},"source":["base_model=keras.applications.xception.Xception(weights=\"imagenet\",include_top=False)\n","avg=keras.layers.GlobalAveragePooling2D()(base_model.output)\n","class_output=keras.layers.Dense(n_classes,activation=\"softmax\")(avg)\n","loc_output=keras.layers.Dense(4)(avg)\n","model=keras.Model(inputs=base_model.input,outputs=[class_output,loc_output])\n","model.compile(loss=[\"sparse_categorical_crossentropy\",\"mse\"],loss_weights=[0.8,0.2],optimizer=optimizer,\n","              metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y13pK161uiEm"},"source":["* 훈련을위해 바운딩박스가 있는 데이터셋이없다.\n","* 클래스 레이블,바운딩 박스와 함께 전처리된 이미지의 배치가 하나의 원소인 데이터셋을 만들어야 한다.\n","* 바운딩박스의 높이와 너비와 수직좌표의 범위를 0에서 1 사이로 정규화 해야한다. 또한 일반적으로 높이와 너비를 직접 예측하지 않고 높이와 너비의 제곱근을 예측한다. 따라서 작은 바운딩박스에서 10픽셀 오차가 큰 바운딩박스보다 더 많은 벌칙을받는다.\n","* mse는 모델을 훈련하기위한 손실함수로 사용할 수 있다. 하지만 바운딩박스의 평가지표는 좋지않다.\n","* 널리 사용되는 지표는 IOU이다.\n","---\n","### 객체 탐지\n","* 과거 사용하던 방법은 하나의 물체를 분류하고 위치를 찾는 분류기를 훈련 한 다음 모든 이미지를 훑는 것이었다.\n","* 이미지를 6*8등의 격자로 나누어서 3*3영역을 지나가는 방식 등.\n","* 이 방식은 매우 쉽지만 동일한 위치에서 여러번 감지한다.\n","  * 따라서 불필요한 박스를 제거위해 사후처리가 필요하다.\n","  * 흔히 NMS 사용\n","    * 존재여부 출력 추가. 시그모이드 사용하고 이지느로스 엔트로피로 출력. 임곗값 이하의 박스를 모두 삭제한다.\n","    * 존재 여부가 가장 높은 바운딩박스를 찾고 이와 많이 중첩된 바운딩 박스를 모두 제거한다.\n","    * 더는 제거할 박스가 없을때까지 2단계를 반복한다.\n","---\n","### 완전 합성곱 신경망.\n","* FCN을 사용하면 CNN을 훨씬 빠르게 이미지에 슬라이딩 가능\n","* CNN 맨 위의 밀집층을 합성곱 층으로 바꿀수있다함.\n","* 맨위 층을 합성곱층으로 출력 피쳐를 만드는 방식.\n","* 밀집층은 특정 크기의 입력을 기대하지만 합성곱은 어떤 크기의 이미지도 처리 가능. 대신 채널을 기대한다.\n","* FCN은 이미지를 한번만 처리하기에 효율적\n","---\n","### YOLO\n","* 2015 제안된 매우 빠르고 정화간 객체 탐지 구조이다. 실시간 가능.\n","* 각 격자 셀마다 5개의 바운딩 박스를 출력한다.\n","* 바운딩박스마다 하나의 존재 여부 점수가 부여된다.\n","  * PASCAL VOC에 훈련되어서 20개의 클래스 확률을 출력한다.\n","  * 따라서 격자 셀마다 45개의 숫자가 출력된다.\n","    * 4개의 좌표를 가진 5개의 바운딩 박스,5개의 존재여부 점수,20개의 클래스 확률이 나온다.\n","  * YOLOv3는 바운딩 박수 중심의 절대좌표를 예측하는 대신 격자 셀에 대한 상대좌표를 예측한다.\n","    * 바운딩 박스의 중심이 격자 셀 안에 있는 것만 예측한다. 바운딩 박스는 격자 밖으로 나갈 수 있다.\n","    * YOLOv3는 로지스틱 활성화함수로 바운딩 박스 좌표가 0과 1사이로 나오게 한다.\n","* YoLov3는 앵커박스라 부르는 5개의 대표 바운딩 박스 크기를 찾는다.\n","  * KNN을 훈련세트 바운딩 박스의 높이와 너비에 적용한다.\n","  * 네트워크가 다른 스케일의 이미지를 사용해서 훈련한다.\n","* 스킵 연결로 CNN과정에서 잃어버린 정보 복원하는 방식이 있다.\n","##### map\n","* 객체 탐지에 널리 사용되는 평가지표.\n","----\n","* 텐서플로를 사용한 여러 YOLO구현이 깃허브에 있다.\n","* SSD,FasterRcnn등은 TF 허브에 있다.\n","---\n","### 시맨틱 분할\n","* 픽셀이 속한 객체의 클래스로 분류된다.\n","* 이작업에서 가장 어려운 점은 CNN을 통과할때 위치정보를 잃는것.\n","* CNN을 FCN으로 변환한다.\n","* 인스턴스 분할은 동일한 클래스 물체를 하나의 덩어리로 합치지 않고 구분한다.\n","  * 이는 Mask R_CNN이다!\n","* 딥러닝을 사용한 컴퓨터 비전분야는 매우 빠르게 발전한다\n","  * 적대적 학습, 이미지 생성, 싱글-샷 학습 등.\n","  *  새로운 네트워크도 있다. 힌던의 캡슐 네트워크"]},{"cell_type":"code","metadata":{"id":"guT838VMtcb5"},"source":[""],"execution_count":null,"outputs":[]}]}